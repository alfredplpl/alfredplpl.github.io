<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Alfred Incrementのまとめサイト">
    <meta name="author" content="alfredplpl">

    <title>プロフィール</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <!-- <link href="css/grid.css" rel="stylesheet">->>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <STYLE type="text/css">
    <!--
      div.resizeimage img{ width: 100%; max-width: 100%; height: auto; }
      .slideshare{
        position:relative;
        width:100%;
        height:0;
        padding-top:75%;
      }
      .slideshare iframe{
        position:absolute;
        top:0;
        left:0;
        width:100%;
        height:100%;
      }
    -->
    </STYLE>

  </head>

  <body class=".no-thank-yu">

      <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Alf's Room</a>
        </div>
      </div>
    </nav>

    <br> <!-- whats this!-->
    <br> <!-- whats this!-->

    <div class="container">

      <div class="page-header">
        <h1>尾崎安範 <br> (Yasunori Ozaki)</h1>
        <p class="lead">このページでは研究活動などをまとめています。</p>
      </div>
              
          
        <h2>研究内容</h2>  
        <div>
            各種センサーから得られた情報から得られた情報を基に人の状態を推定し、
            その人の状態に合わせて行動することで、
            実世界の人間と人間らしくインタラクションするロボットを開発しています。
            またその開発のために、人間らしさを工学として利用できるようにする研究も
            行っています。現在は、画像認識や音声認識、
            音声合成などの機械学習技術を連携制御することで実現しています。
        </div>

        <h3>空気を読むロボット -ロボット展示員は通行人に不快感を与えず、こちらに興味を引かせられるのか？- [12]</h3>  
        <div class="row">
          <div class="col-xs-0 col-md-4 col-lg-3"></div>
          <div class="col-xs-12 col-md-6 col-lg-6"> 
            <div class="resizeimage"><img src="img/IROS_problem_statement.png" alt="problem statement" ></div>
          </div> 
          <div class="col-xs-0 col-md-4 col-lg-3"></div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12 ">   
            本研究では、コミュニケーションロボットが通行人に不快感を与えずに挨拶をして注意を引く方法を開発することを目的としています。
            近年、人ではなくコミュニケーションロボットが受付や案内、展示などのサービスを行うことが多くなっています。
            例えば、ロボットの展示者は、ロボットの所有者が宣伝している商品を説明することができます。
            しかし、ロボットの突然の挨拶は、通行人を驚かせ、通行人に不快感を与える可能性があります。
          </div>
        </div>
        <div class="row">
          <div class="col-xs-0 col-md-3 col-lg-3"></div>
          <div class="col-xs-6 col-md-3 col-lg-3"> 
            <div class="resizeimage"><img src="img/IROS_block.png" alt="proposed method" ></div>
          </div> 
          <div class="col-xs-6 col-md-3 col-lg-3">
            <div class="resizeimage"><img src="img/IROS_robot_attracts_person.jpg" alt="proposed method" ></div>
          </div>
          <div class="col-xs-0 col-md-3 col-lg-3"></div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12 ">   
            そのため、コミュニケーションロボットは、通行人が直面する状況に応じて、自分のマナーを適応させる必要があります。
            私たちは、この要求を満たすための手法を、関連研究の結果に基づいて開発しました。
            本研究では、ロボットが通行人に不快感を与えることなく、挨拶をして注意を引くことができる手法「ユーザ中心強化学習」を提案しています。
            オフィスのエントランスというフィールドでの実験の結果、本手法がこの要求を満たすことが実証されました。
          </div>
        </div>

        <h3>通行者の行動モデルに基づいてサービス利用を予測するシステム[9][10]</h3>
        <div class="row">
          <div class="col-xs-0 col-md-2 col-lg-2"> 
          </div>
          <div class="col-xs-12 col-md-8 col-lg-8"> 
              <div class="resizeimage"><img src="img/muller.png" alt="fire" ></div>
          </div>
          <div class="col-xs-0 col-md-2 col-lg-2"> 
          </div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12"> 
            本研究の目的は，ロボットの前を通りかかった歩行者がそのロボットが提供するサービスを利用するか、その意志決定を予測する方法を実現することです。
            また、その手法がもたらす心理的影響を明らかにすることです。
            この目的に達成するために、サービスを利用する人間の行動を従来の研究から数理モデル化し、センサデータから人間の行動をモデルに合致するかシミュレートする方法を取りました。
            具体的には、センサデータから歩行者の位置や顔向きを推定し、その位置や顔向きから人間の状態を予測、人間の状態が利用している状態にあると予測し、そうでない場合、利用しないと予測する方法を取ります。
            検証実験の結果、統制された環境では、利用意志をすべて正確に予測でき、実環境に基づいたシミュレーションでは他の手法に比べ高い精度で利用意志を予測できることが統計的にわかりました。また、この手法に基づいて、利用意志があると推定される歩行者に音声で呼びかけた結果、他の手法に比べ、不快感を与えにくい特徴があることが新たにわかりました。
          </div>
        </div>
        
        <h3>通行者の行動モデルに基づいてサービス利用を促すバーチャルエージェント[2][8]</h3>
        <div class="row">
          <div class="col-xs-3 col-md-2 col-lg-1"> 
            <div class="resizeimage"><img src="img/nep.png" alt="fire" ></div>
          </div>
          <div class="col-xs-9 col-md-10 col-lg-11"> 
            本研究の目的は，サービス利用を促すバーチャルエージェントを備えたインタラクティブサイネージを実現することです。
            この目的に達成するために、バーチャルエージェントが通行者に利用を音声にて呼びかけるインタラクティブサイネージを構築しました。
            サイネージを利用する割合とサイネージに対する通行者の印象の二つの観点からサイネージの有効性を現場にて実験により検証しました．
            検証実験の結果，エージェントが呼びかけを行わない場合に比べ，エージェントの横を通過している通行者に対してエージェントが呼びかけを行うと
            エージェントとインタラクションする通行者の割合は多くなる傾向が見られました。
          </div>
        </div>

        <hr>
        <h2>発表文献</h2> 
        <h3>ジャーナル/論文誌</h3> 
        <ul>
          <li>[3] Yusuke Sugano, Yasunori Ozaki, Hiroshi Kasai, Keisuke Ogaki and Yoichi Sato, "Image Preference Estimation with a Data-driven Approach: A Comparative Study between Gaze and Image Features," Journal of Eye Movement Research, vol. 7, num. 3, 2014.</li>
        </ul>
        <h3>査読付き国際会議/国内会議</h3>
        <ul>
          <li>[14] Yuki Tamaru, Yasunori Ozaki, Yuki Okafuji, Jun Baba, Junya Nakanishi, Yuichiro Yoshikawa, "3D Head-Position Prediction in First-Person View by Considering Head Pose for Human-Robot Eye Contact", HRI 2022 (LBR Accepted, preprint: <a href="https://arxiv.org/abs/2103.06417">arXiv</a>) </li>
          <li>[13] Okafuji, Yuki and Ozaki, Yasunori and Baba, Jun and Kitahara, Asano and Nakanishi, Junya and Ogawa, Kohei and Yoshikawa, Yuichiro and Ishiguro, Hiroshi, "Please Listen to Me: How to Make Passersby Stop by a Humanoid Robot in a Shopping Mall", HRI 2020 (LBR, acceptance rate: 88%)</li>
          <li>[12] Yasunori Ozaki, Tatsuya Ishihara, Narimune Matsumura, Tadashi Nunobiki, "Can User-Centered Reinforcement Learning Allow a Robot to Attract Passersby without Causing Discomfort?", IROS 2019 (acceptance rate: 44.9%, preprint: <a href="https://arxiv.org/abs/1903.05881">arXiv</a> )</li>
          <li>[10] Ozaki, Yasunori; Ishihara, Tatsuya; Matsumura, Narimune; Nunobiki, Tadashi; Yamada, Tomohiro, "Decision-Making Prediction for Human-Robot Engagement between Pedestrian and Robot Receptionist", IEEE RO-MAN 2018 (Oral, acceptance rate: 40-50%)</li>
          <li>[7] Ozaki, Yasunori, Aoki, Ryosuke, Kimura, Toshitaka,.Takashima, Youichi Yamada, Tomohiro, "Characterizing Multi EMG Channels Using Non-Negative Matrix Factorization for Driver Swings", IEEE EMBC 2016 (Poster, acceptance rate: 30-50%)</li>
          <li>[11] 矢野裕季, 東風上奏絵, 中野将尚, 尾崎安範, 佐藤大貴, 倉橋孝雄, 越地弘順,肥後直樹, 椿俊光, 布引純史(NTT), "電動車椅子のための実環境における歩行者回避領域の評価手法", 第24回ロボティクスシンポジア, pp.299-300, 2019</li>
        </ul>

        <h3>その他の文献</h3> 
        <ul>
          <li>[15] Yuki Okafuji, Yasunori Ozaki, Jun Baba, Junya Nakanishi, Kohei Ogawa, Yuichiro Yoshikawa, Hiroshi Ishiguro, “Behavioral assessment of a humanoid robot when attracting pedestrians in a mall,” arXiv, 2021 (preprint: <a href="https://arxiv.org/abs/2109.02771">arXiv</a>) </li>
          <li>[9] 尾崎安範, 石原達也, 松村成宗,  布引純史,  "受付ロボットに対する通行者が抱く対話意志の予測とその心理的効果", CNR 2018</li>
          <li>[8] 尾崎安範, 石原達也, 前田航洋, 鏡明彦, 松村成宗, 望月崇由, 布引純史, 山田智広, "通行者の行動モデルに基づいてサービス利用を促すバーチャルエージェントを備えたインタラクティブサイネージ", CNR 2017</li>
          <li>[6] 尾崎安範、青木良輔、木村俊貴、高嶋洋一、山田智広、"ゴルフスイングにおける筋活動センサデータからの非負値行列因子分解による特徴抽出" MBE, 2016</li>
          <li>[5] 尾崎安範, 青木良輔, 松村成宗, 高嶋洋一, 山田智広, "運動学習を促進する力触覚インタラクション技術に関する検討",  クラウドネットワークロボット研究会(CNR), 2015</li>
          <li>[4] 尾崎安範, 菅野裕介, 佐藤洋一, "視線情報と画像特徴に基づく画像の選好推定", パターン認識・メディア理解研究会（PRMU）, 2014</li>
          <li>[2] 尾崎安範,　"目と目で通じ合う初音ミク －視線と微笑みのインタラクション－," あの人の研究論文集, Vol.3, No.1, 2012 </li>
          <li>(ネタ論文ですが、査読付きで採択率は約30%でした。投稿者はほぼ東大生でした。)</li>
          <li>[1] 尾崎安範, 出口大輔, 高橋友和, 井手一郎, 村瀬 洋, "印象に基づく属性による顔画像の検索に関する検討," 電子情報通信学会 総合大会, 2012 </li>
        </ul>

        <h2>学術的活動</h2> 
        <ul>
          <li>座長経験: AVIC 2021 Session Chair</li>
          <li>査読経験: 電気学会 論文誌, HAI 2021</li>
        </ul>

        <h2>広報活動</h2>
        <h3>メディア掲載</h3>
          <ol>
            <li>"ロボットが呼び込み　大阪で実証実験、商業施設を活用", 日本経済新聞電子版, 2019, <a target=”_blank” href="https://www.nikkei.com/article/DGXMZO47486830Y9A710C1LKA000/">日経新聞のサイトへ</a></li>
            <li>"夜の東京でロボ実験、小池都知事が視察　調理や運搬", 日本経済新聞電子版, 2019, <a target=”_blank” href="https://www.nikkei.com/article/DGXMZO48874080S9A820C1L83000/" >日経新聞のサイトへ</a></li>
            <li>"A new approach allows robots to attract passersby without causing them discomfort", Tech Xplore, 2019,<a target=”_blank” href="https://techxplore.com/news/2019-03-approach-robots-passersby-discomfort.html">ニュースサイトへ </a> </li>
            <li>"NTT、「おもてなし」の最新技術を研究・開発中、2020年に向けて", INTERNET Watch, 2015, <a href="https://internet.watch.impress.co.jp/docs/event/689310.html">ニュースサイトへ</a></li>
            <ul>
              <li>補足: パワードスーツのくだりが私の担当になります。</li>
            </ul>
          </ol>
        <h3>プレスリリース</h3>
          <ol>
            <li>"報道発表資料　ロボットが案内すると商品の売上がアップ!?南港ATCでロボットによる接客・広告の実証実験を実施します", 大阪市, 
              <a target=”_blank” href="https://www.city.osaka.lg.jp/hodoshiryo/keizaisenryaku/0000474460.html">大阪市のページへ</a></li>
          </ol>

        <h2>公開された特許</h2>
        <ul>
          <li>[特許9] 尾崎  安範 他, "情報出力装置、方法およびプログラム", 特開2020-24517</li>     
          <li>[特許8] 尾崎  安範 他, "情報出力装置、方法およびプログラム", 特開2019-128910</li>
          <li>[特許7] 尾崎  安範 他, "情報出力装置、方法およびプログラム", 特開2019-128557</li>
          <li>[特許6] 尾崎  安範 他, "行動状態推定装置、行動状態推定方法及びそのプログラム", 特開2019-087175</li>
          <li>[特許5] 尾崎  安範 他, "筋活動可聴化装置、筋活動可聴化方法およびプログラム", 特開2018-023445</li>
          <li>[特許4] 尾崎  安範 他, "筋活動解析装置、方法およびプログラム", 特開2018-015405</li>
          <li>[特許3] 尾崎  安範 他, "筋活動推定装置、方法およびプログラム", 特開2018-015408</li>
          <li>[特許2] 尾崎  安範 他, "力感フィードバック装置", 特開2018-020002</li>
          <li>[特許1] 杉山  弘晃, 目黒  豊美, 大和  淳司, 山田  智広, 望月  崇由, 松元  崇裕, 尾崎  安範, 吉川  雄一郎, 石黒  浩,  "対話方法、対話システム、対話装置、およびプログラム", 特開2017-207693</li>
        </ul>

        <h2>ポートフォリオ</h2>
        <p>学生時代や社会人で作成した主な作品の概要をスライドにまとめました。</p>
        <div class="row">
          <div class="col-xs-0 col-md-2 col-lg-2"> </div>
          <div class="col-xs-12 col-md-8 col-lg-8"> 
            <div class="slideshare"> 
              <iframe src="https://www.slideshare.net/slideshow/embed_code/key/by2XDH1AdEaQ5Q" width="640" height="480" frameborder="0" style="border:0" allowfullscreen> </iframe>
            </div>
          </div>
          <div class="col-xs-0 col-md-2 col-lg-2"> </div>
        </div>

        <h2>連絡先</h2>
        <div>
          E-mail: ozaki.yasunori (at) outlook.com <br>
          (at)を@に変更してください。
        </div>
        
        
      <hr>
      <footer>
        <p>&copy; 2019 Yasunori Ozaki powered by <a href="https://rinhoshizo.la/"> Rin </a> Based on Bootstrap .</p>
      </footer>
    </div> <!-- /container -->


    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
