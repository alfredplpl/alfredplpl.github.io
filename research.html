<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Alfred Incrementのまとめサイト">
    <meta name="author" content="alfredplpl">

    <title>プロフィール</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <!-- <link href="css/grid.css" rel="stylesheet">->>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <STYLE type="text/css">
    <!--
      div.resizeimage img{ width: 100%; max-width: 100%; height: auto; }
      .slideshare{
        position:relative;
        width:100%;
        height:0;
        padding-top:75%;
      }
      .slideshare iframe{
        position:absolute;
        top:0;
        left:0;
        width:100%;
        height:100%;
      }
    -->
    </STYLE>

  </head>

  <body class=".no-thank-yu">

      <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Alf's Room</a>
        </div>
      </div>
    </nav>

    <br> <!-- whats this!-->
    <br> <!-- whats this!-->

    <div class="container">

      <div class="page-header">
        <h1>尾崎安範 <br> (Yasunori Ozaki)</h1>
        <p class="lead">このページでは研究活動などをまとめています。</p>
      </div>
              
          
        <h2>研究内容</h2>  
        <div>
            作風を模倣する画像生成AIに関する研究開発を行っております。
            画像だけではなく、動画生成AIも開発しております。
            また、各種センサーから得られた情報から得られた情報を基に人の状態を推定し、
            その人の状態に合わせて行動することで、
            実世界の人間と人間らしくインタラクションするロボットも開発しています。
        </div>

        <h3>特定の作風を模倣する画像・動画生成AIの技術面と倫理面の研究 [16, 17]</h3>  
        <div class="row">
          <div class="col-xs-0 col-md-4 col-lg-3"></div>
          <div class="col-xs-12 col-md-6 col-lg-6"> 
            <div class="resizeimage"><img src="img/fake.png" alt="problem statement" ></div>
          </div> 
          <div class="col-xs-0 col-md-4 col-lg-3"></div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12 ">   
            個人的な研究の末、拡散モデルやフローといった高度な画像生成AIが特定の作風を模倣できることを発見しました。
            本研究では、その模倣が社会に与える影響を中長期的に調べています。
            一つの応用として、AIいらすとやがあります。これはいらすとやを運営するみふねたかしさんの協力のもと、
            いらすとやの作風を模倣したAIで作家に利益還元するという現象の活用法を示しました。
            現在も倫理的な研究を進めています。
          </div>
        </div>
        <h3>空気を読むロボット -ロボット展示員は通行人に不快感を与えず、こちらに興味を引かせられるのか？- [12]</h3>  
        <div class="row">
          <div class="col-xs-0 col-md-4 col-lg-3"></div>
          <div class="col-xs-12 col-md-6 col-lg-6"> 
            <div class="resizeimage"><img src="img/IROS_problem_statement.png" alt="problem statement" ></div>
          </div> 
          <div class="col-xs-0 col-md-4 col-lg-3"></div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12 ">   
            本研究では、コミュニケーションロボットが通行人に不快感を与えずに挨拶をして注意を引く方法を開発することを目的としています。
            近年、人ではなくコミュニケーションロボットが受付や案内、展示などのサービスを行うことが多くなっています。
            例えば、ロボットの展示者は、ロボットの所有者が宣伝している商品を説明することができます。
            しかし、ロボットの突然の挨拶は、通行人を驚かせ、通行人に不快感を与える可能性があります。
          </div>
        </div>
        <div class="row">
          <div class="col-xs-0 col-md-3 col-lg-3"></div>
          <div class="col-xs-6 col-md-3 col-lg-3"> 
            <div class="resizeimage"><img src="img/IROS_block.png" alt="proposed method" ></div>
          </div> 
          <div class="col-xs-6 col-md-3 col-lg-3">
            <div class="resizeimage"><img src="img/IROS_robot_attracts_person.jpg" alt="proposed method" ></div>
          </div>
          <div class="col-xs-0 col-md-3 col-lg-3"></div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12 ">   
            そのため、コミュニケーションロボットは、通行人が直面する状況に応じて、自分のマナーを適応させる必要があります。
            私たちは、この要求を満たすための手法を、関連研究の結果に基づいて開発しました。
            本研究では、ロボットが通行人に不快感を与えることなく、挨拶をして注意を引くことができる手法「ユーザ中心強化学習」を提案しています。
            オフィスのエントランスというフィールドでの実験の結果、本手法がこの要求を満たすことが実証されました。
          </div>
        </div>

        <h3>通行者の行動モデルに基づいてサービス利用を予測するシステム[9][10]</h3>
        <div class="row">
          <div class="col-xs-0 col-md-2 col-lg-2"> 
          </div>
          <div class="col-xs-12 col-md-8 col-lg-8"> 
              <div class="resizeimage"><img src="img/muller.png" alt="fire" ></div>
          </div>
          <div class="col-xs-0 col-md-2 col-lg-2"> 
          </div>
        </div>
        <div class="row">
          <div class="col-xs-12 col-md-12 col-lg-12"> 
            本研究の目的は，ロボットの前を通りかかった歩行者がそのロボットが提供するサービスを利用するか、その意志決定を予測する方法を実現することです。
            また、その手法がもたらす心理的影響を明らかにすることです。
            この目的に達成するために、サービスを利用する人間の行動を従来の研究から数理モデル化し、センサデータから人間の行動をモデルに合致するかシミュレートする方法を取りました。
            具体的には、センサデータから歩行者の位置や顔向きを推定し、その位置や顔向きから人間の状態を予測、人間の状態が利用している状態にあると予測し、そうでない場合、利用しないと予測する方法を取ります。
            検証実験の結果、統制された環境では、利用意志をすべて正確に予測でき、実環境に基づいたシミュレーションでは他の手法に比べ高い精度で利用意志を予測できることが統計的にわかりました。また、この手法に基づいて、利用意志があると推定される歩行者に音声で呼びかけた結果、他の手法に比べ、不快感を与えにくい特徴があることが新たにわかりました。
          </div>
        </div>
        
        <h3>通行者の行動モデルに基づいてサービス利用を促すバーチャルエージェント[2][8]</h3>
        <div class="row">
          <div class="col-xs-3 col-md-2 col-lg-1"> 
            <div class="resizeimage"><img src="img/nep.png" alt="fire" ></div>
          </div>
          <div class="col-xs-9 col-md-10 col-lg-11"> 
            本研究の目的は，サービス利用を促すバーチャルエージェントを備えたインタラクティブサイネージを実現することです。
            この目的に達成するために、バーチャルエージェントが通行者に利用を音声にて呼びかけるインタラクティブサイネージを構築しました。
            サイネージを利用する割合とサイネージに対する通行者の印象の二つの観点からサイネージの有効性を現場にて実験により検証しました．
            検証実験の結果，エージェントが呼びかけを行わない場合に比べ，エージェントの横を通過している通行者に対してエージェントが呼びかけを行うと
            エージェントとインタラクションする通行者の割合は多くなる傾向が見られました。
          </div>
        </div>

        <hr>
        <h2>発表文献</h2> 
        <h3>ジャーナル/論文誌</h3> 
        <ul>
          <li>[15] Yuki Okafuji, Yasunori Ozaki, Jun Baba, Junya Nakanishi, Kohei Ogawa, Yuichiro Yoshikawa, Hiroshi Ishiguro, “Behavioral assessment of a humanoid robot when attracting pedestrians in a mall,” International Journal of Social Robotics, 2022 (preprint: <a href="https://arxiv.org/abs/2109.02771">arXiv</a>) </li>
          <li>[3] Yusuke Sugano, Yasunori Ozaki, Hiroshi Kasai, Keisuke Ogaki and Yoichi Sato, "Image Preference Estimation with a Data-driven Approach: A Comparative Study between Gaze and Image Features," Journal of Eye Movement Research, vol. 7, num. 3, 2014.</li>
        </ul>
        <h3>査読付き国際会議/国内会議</h3>
        <ul>
          <li>[14] Yuki Tamaru, Yasunori Ozaki, Yuki Okafuji, Jun Baba, Junya Nakanishi, Yuichiro Yoshikawa, "3D Head-Position Prediction in First-Person View by Considering Head Pose for Human-Robot Eye Contact", HRI 2022 (LBR Accepted, preprint: <a href="https://arxiv.org/abs/2103.06417">arXiv</a>) </li>
          <li>[13] Okafuji, Yuki and Ozaki, Yasunori and Baba, Jun and Kitahara, Asano and Nakanishi, Junya and Ogawa, Kohei and Yoshikawa, Yuichiro and Ishiguro, Hiroshi, "Please Listen to Me: How to Make Passersby Stop by a Humanoid Robot in a Shopping Mall", HRI 2020 (LBR, acceptance rate: 88%)</li>
          <li>[12] Yasunori Ozaki, Tatsuya Ishihara, Narimune Matsumura, Tadashi Nunobiki, "Can User-Centered Reinforcement Learning Allow a Robot to Attract Passersby without Causing Discomfort?", IROS 2019 (acceptance rate: 44.9%, preprint: <a href="https://arxiv.org/abs/1903.05881">arXiv</a> )</li>
          <li>[10] Ozaki, Yasunori; Ishihara, Tatsuya; Matsumura, Narimune; Nunobiki, Tadashi; Yamada, Tomohiro, "Decision-Making Prediction for Human-Robot Engagement between Pedestrian and Robot Receptionist", IEEE RO-MAN 2018 (Oral, acceptance rate: 40-50%)</li>
          <li>[7] Ozaki, Yasunori, Aoki, Ryosuke, Kimura, Toshitaka,.Takashima, Youichi Yamada, Tomohiro, "Characterizing Multi EMG Channels Using Non-Negative Matrix Factorization for Driver Swings", IEEE EMBC 2016 (Poster, acceptance rate: 30-50%)</li>
          <li>[11] 矢野裕季, 東風上奏絵, 中野将尚, 尾崎安範, 佐藤大貴, 倉橋孝雄, 越地弘順,肥後直樹, 椿俊光, 布引純史(NTT), "電動車椅子のための実環境における歩行者回避領域の評価手法", 第24回ロボティクスシンポジア, pp.299-300, 2019</li>
        </ul>

        <h3>その他の文献</h3> 
        <ul>
          <li>[19] 尾崎安範，石原昌文，富平準喜, "日本語入力にネイティブ対応したテキストからの動画生成のフルスクラッチ開発と公開", Jxiv, 2025 (<a href="https://jxiv.jst.go.jp/index.php/jxiv/preprint/view/1248">Jxiv</a>)</li>
          <li>[18] 尾崎安範，三嶋隆史，富平準喜, "CommonArt ～ 国産大規模言語モデルによる透明性の高い画像生成用拡散トランスフォーマー ～", PRMU, 2024 (<a href="https://jxiv.jst.go.jp/index.php/jxiv/preprint/view/936">Jxiv</a>)</li>
          <li>[17] 尾崎安範, "特定の作家の作風に酷似した顔アイコンを創作する符号統合拡散モデル ～ 創作するイラスト深層生成モデルを活用する受容性の調査 ～", PRMU, 2022</li>
          <li>[16] 尾崎安範, "特定の作家の作風に酷似した顔アイコンを創作する拡散モデル(仮題)", preprint <a href="ddpm_preprint_v2.pdf">v2</a> <a href="ddpm_preprint_v1.pdf">v1</a>, <a href="ddpm_analysis_preprint_v1.pdf">analysis</a></li>
          <li>[9] 尾崎安範, 石原達也, 松村成宗,  布引純史,  "受付ロボットに対する通行者が抱く対話意志の予測とその心理的効果", CNR 2018</li>
          <li>[8] 尾崎安範, 石原達也, 前田航洋, 鏡明彦, 松村成宗, 望月崇由, 布引純史, 山田智広, "通行者の行動モデルに基づいてサービス利用を促すバーチャルエージェントを備えたインタラクティブサイネージ", CNR 2017</li>
          <li>[6] 尾崎安範、青木良輔、木村俊貴、高嶋洋一、山田智広、"ゴルフスイングにおける筋活動センサデータからの非負値行列因子分解による特徴抽出" MBE, 2016</li>
          <li>[5] 尾崎安範, 青木良輔, 松村成宗, 高嶋洋一, 山田智広, "運動学習を促進する力触覚インタラクション技術に関する検討",  クラウドネットワークロボット研究会(CNR), 2015</li>
          <li>[4] 尾崎安範, 菅野裕介, 佐藤洋一, "視線情報と画像特徴に基づく画像の選好推定", パターン認識・メディア理解研究会（PRMU）, 2014</li>
          <li>[2] 尾崎安範, "目と目で通じ合う初音ミク －視線と微笑みのインタラクション－," あの人の研究論文集, Vol.3, No.1, 2012 </li>
          <li>(ネタ論文ですが、査読付きで採択率は約30%でした。投稿者はほぼ東大生でした。)</li>
          <li>[1] 尾崎安範, 出口大輔, 高橋友和, 井手一郎, 村瀬 洋, "印象に基づく属性による顔画像の検索に関する検討," 電子情報通信学会 総合大会, 2012 </li>
        </ul>

        <h2>研究開発の実用化</h2>
        <ul>
          <li><a href="https://aisozai.com/irasutoya">AIいらすとや</a></li>
          <li><a href="https://aisozai.com/loosedrawing">Loose AI</a></li>
        </ul>

        <h2>学術的活動</h2> 
        <ul>
          <li>座長経験: AVIC 2021 Session Chair</li>
          <li>査読経験: MIRU 2024, MIRU 2023, MIRU 2022 (査読賞), ICPR 2022, HAI 2021, 電気学会 論文誌</li>
          <li>招待講演: <a href="https://www.docswell.com/s/alfredplpl/KJLQ9N-2023-07-23-165655">エンタテイメント情報学 2023（福知山公立大学）</a>, <a href="https://www.docswell.com/s/alfredplpl/KL643Z-2022-07-19-172629">信号とシステム 2022（阪大）</a> </li>
        </ul>
        <h2>研究助成</h2> 
        <h3>代表</h3>
        <ul>
          <li>NEDO GENIAC, "動画生成AI基盤モデルと動画生成AIプラットフォームの開発", 2025, <a href="https://www.meti.go.jp/policy/mono_info_service/geniac/selection_2/index.html">link</a></li>
        </ul>

        <h2>公益的活動</h2> 
        <ul>
          <li>"AI時代の知的財産権検討会（第２回）", 内閣府, 2023, <a target=”_blank” href="https://www.kantei.go.jp/jp/singi/titeki2/ai_kentoukai/gijisidai/dai2/index.html">内閣府のサイトへ</a></li>
        </ul>

        <h2>広報活動</h2>
        <h3>メディア掲載</h3>
          <ol>
            <li>"日本初の日本語対応の動画生成AI基盤モデル「AIdeaLab VideoJP」無償公開！　商用利用可、ライセンスに考慮しゼロから動画学習", CGWORLD, 2025, <a target=”_blank” href="https://cgworld.jp/flashnews/202502-AIdeaLab-VideoJP.html">CGWORLDのサイトへ</a></li>
            <li>"キャプション付き画像のデータ約1000万枚、AI Picassoが無償公開　「著作権に配慮、AIモデル開発に利用して」", ITmedia, 2024, <a target=”_blank” href="https://www.itmedia.co.jp/aiplus/articles/2407/31/news121.html">ITmediaのサイトへ</a></li>
            <li>"商用利用OKの画像生成AI「Emi」公開　クリエイターと対話して開発、無断転載画像不使用", ITmedia, 2023, <a target=”_blank” href="https://www.itmedia.co.jp/news/articles/2309/27/news158.html">ITmediaのサイトへ</a></li>
            <li>"「いらすとや」風画像を無限に生成　「AIいらすとや」商用利用可に　有料サービス化", ITmedia, 2023, <a target=”_blank” href="https://www.itmedia.co.jp/news/articles/2308/10/news171.html">ITmediaのサイトへ</a></li>
            <li>"ロボットが困っていると通行人は立ち止まる。サイバーエージェントと阪大が明らかにしたこと", ニュースイッチ, 2022, <a target=”_blank” href="https://newswitch.jp/p/32937">ニュースイッチのサイトへ</a></li>
            <li>"ロボットが呼び込み　大阪で実証実験、商業施設を活用", 日本経済新聞電子版, 2019, <a target=”_blank” href="https://www.nikkei.com/article/DGXMZO47486830Y9A710C1LKA000/">日経新聞のサイトへ</a></li>
            <li>"夜の東京でロボ実験、小池都知事が視察　調理や運搬", 日本経済新聞電子版, 2019, <a target=”_blank” href="https://www.nikkei.com/article/DGXMZO48874080S9A820C1L83000/" >日経新聞のサイトへ</a></li>
            <li>"A new approach allows robots to attract passersby without causing them discomfort", Tech Xplore, 2019,<a target=”_blank” href="https://techxplore.com/news/2019-03-approach-robots-passersby-discomfort.html">ニュースサイトへ </a> </li>
            <li>"NTT、「おもてなし」の最新技術を研究・開発中、2020年に向けて", INTERNET Watch, 2015, <a href="https://internet.watch.impress.co.jp/docs/event/689310.html">ニュースサイトへ</a></li>
            <ul>
              <li>補足: パワードスーツのくだりが私の担当になります。</li>
            </ul>
          </ol>
        <h3>プレスリリース</h3>
          <ol>
            <li>"AI Lab、ロボティクス分野の主要ジャーナル「International Journal of Social Robotics」にて論文採択 ー 自律声掛けロボットにおける対話開始手法を検討 ー", サイバーエージェント, 
              <a target=”_blank” href="https://www.cyberagent.co.jp/news/detail/id=27780">サイバーエージェントのページへ</a></li>
            <li>"報道発表資料　ロボットが案内すると商品の売上がアップ!?南港ATCでロボットによる接客・広告の実証実験を実施します", 大阪市, 
              <a target=”_blank” href="https://www.city.osaka.lg.jp/hodoshiryo/keizaisenryaku/0000474460.html">大阪市のページへ</a></li>
          </ol>

        <h2>公開された特許</h2>
        <ul>
          <li>[特許9] 尾崎  安範 他, "情報出力装置、方法およびプログラム", 特開2020-24517</li>     
          <li>[特許8] 尾崎  安範 他, "情報出力装置、方法およびプログラム", 特開2019-128910</li>
          <li>[特許7] 尾崎  安範 他, "情報出力装置、方法およびプログラム", 特開2019-128557</li>
          <li>[特許6] 尾崎  安範 他, "行動状態推定装置、行動状態推定方法及びそのプログラム", 特開2019-087175</li>
          <li>[特許5] 尾崎  安範 他, "筋活動可聴化装置、筋活動可聴化方法およびプログラム", 特開2018-023445</li>
          <li>[特許4] 尾崎  安範 他, "筋活動解析装置、方法およびプログラム", 特開2018-015405</li>
          <li>[特許3] 尾崎  安範 他, "筋活動推定装置、方法およびプログラム", 特開2018-015408</li>
          <li>[特許2] 尾崎  安範 他, "力感フィードバック装置", 特開2018-020002</li>
          <li>[特許1] 杉山  弘晃, 目黒  豊美, 大和  淳司, 山田  智広, 望月  崇由, 松元  崇裕, 尾崎  安範, 吉川  雄一郎, 石黒  浩,  "対話方法、対話システム、対話装置、およびプログラム", 特開2017-207693</li>
        </ul>

        <h2>ポートフォリオ</h2>
        <p>学生時代や社会人で作成した主な作品の概要をスライドにまとめました。</p>
        <div class="row">
          <div class="col-xs-0 col-md-2 col-lg-2"> </div>
          <div class="col-xs-12 col-md-8 col-lg-8"> 
            <div class="slideshare"> 
              <iframe src="https://www.slideshare.net/slideshow/embed_code/key/by2XDH1AdEaQ5Q" width="640" height="480" frameborder="0" style="border:0" allowfullscreen> </iframe>
            </div>
          </div>
          <div class="col-xs-0 col-md-2 col-lg-2"> </div>
        </div>

        <h2>連絡先</h2>
        <div>
          E-mail: ozaki.yasunori (at) outlook.com <br>
          (at)を@に変更してください。
        </div>
        
        
      <hr>
      <footer>
        <p>&copy; 2019 Yasunori Ozaki powered by <a href="https://rinhoshizo.la/"> Rin </a> Based on Bootstrap .</p>
      </footer>
    </div> <!-- /container -->


    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
